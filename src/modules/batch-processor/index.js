import fs from 'fs-extra';
import path from 'path';
import chalk from 'chalk';

const BATCH_PROCESSOR_DIR = path.join(process.cwd(), '.project-data', 'batch-processor');
const BATCH_PROCESSOR_CONFIG_FILE = path.join(BATCH_PROCESSOR_DIR, 'batch-processor-config.json');
const BATCH_PROCESSOR_REPORT_FILE = path.join(BATCH_PROCESSOR_DIR, 'batch-processor-report.json');

/**
 * ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“ˆ
 * ì—¬ëŸ¬ URLì„ í•œ ë²ˆì— ë¶„ì„í•˜ê³  ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
 */
class BatchProcessor {
  constructor() {
    this.ensureDirectories();
    this.jobs = new Map();
  }

  ensureDirectories() {
    if (!fs.existsSync(BATCH_PROCESSOR_DIR)) {
      fs.mkdirSync(BATCH_PROCESSOR_DIR, { recursive: true });
    }
  }

  async loadConfig() {
    try {
      if (fs.existsSync(BATCH_PROCESSOR_CONFIG_FILE)) {
        return await fs.readJson(BATCH_PROCESSOR_CONFIG_FILE);
      }
      return this.getDefaultConfig();
    } catch (error) {
      return this.getDefaultConfig();
    }
  }

  getDefaultConfig() {
    return {
      enabled: true,
      maxConcurrent: 5,
      retryAttempts: 3,
      retryDelay: 1000
    };
  }

  async processFromFile(filePath, options = {}) {
    try {
      console.log(chalk.blue.bold(`\nğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: ${filePath}\n`));

      const urls = await this.loadUrlsFromFile(filePath);
      const config = await this.loadConfig();

      const jobId = `job-${Date.now()}`;
      const job = {
        id: jobId,
        status: 'running',
        total: urls.length,
        completed: 0,
        failed: 0,
        results: [],
        startTime: new Date().toISOString(),
        endTime: null
      };

      this.jobs.set(jobId, job);

      console.log(chalk.blue(`ì´ ${urls.length}ê°œ URL ì²˜ë¦¬ ì‹œì‘\n`));

      // ë³‘ë ¬ ì²˜ë¦¬ (ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ)
      const chunks = this.chunkArray(urls, config.maxConcurrent);
      
      for (const chunk of chunks) {
        const promises = chunk.map(url => this.processUrl(url, jobId, options));
        await Promise.allSettled(promises);
      }

      job.status = 'completed';
      job.endTime = new Date().toISOString();

      // ë¦¬í¬íŠ¸ ì €ì¥
      await this.saveJobReport(job);

      console.log(chalk.green(`\nâœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ`));
      console.log(chalk.blue(`ì™„ë£Œ: ${job.completed}/${job.total}`));
      console.log(chalk.blue(`ì‹¤íŒ¨: ${job.failed}/${job.total}\n`));

      return job;
    } catch (error) {
      console.error(chalk.red(`âŒ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: ${error.message}`));
      throw error;
    }
  }

  async loadUrlsFromFile(filePath) {
    const ext = path.extname(filePath).toLowerCase();
    const content = await fs.readFile(filePath, 'utf-8');

    if (ext === '.json') {
      const data = JSON.parse(content);
      return Array.isArray(data) ? data : data.urls || [];
    } else if (ext === '.csv') {
      return this.parseCSV(content);
    } else {
      // í…ìŠ¤íŠ¸ íŒŒì¼: í•œ ì¤„ì— í•˜ë‚˜ì”© URL
      return content.split('\n').filter(line => line.trim().length > 0);
    }
  }

  parseCSV(content) {
    const lines = content.split('\n').filter(line => line.trim().length > 0);
    const urls = [];
    
    // ì²« ë²ˆì§¸ ì¤„ì€ í—¤ë”ì¼ ìˆ˜ ìˆìŒ
    for (let i = 1; i < lines.length; i++) {
      const line = lines[i];
      const columns = line.split(',');
      // ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ URLë¡œ ê°€ì •
      if (columns[0] && columns[0].trim().startsWith('http')) {
        urls.push(columns[0].trim());
      }
    }
    
    return urls;
  }

  chunkArray(array, size) {
    const chunks = [];
    for (let i = 0; i < array.length; i += size) {
      chunks.push(array.slice(i, i + size));
    }
    return chunks;
  }

  async processUrl(url, jobId, options = {}) {
    const job = this.jobs.get(jobId);
    if (!job) return;

    try {
      console.log(chalk.blue(`ì²˜ë¦¬ ì¤‘: ${url}`));

      // ë¶„ì„ ëª¨ë“ˆ ì„ íƒ
      const { default: aioModule } = await import('../aio/index.js');
      const result = await aioModule.comprehensiveAnalysis(url);

      job.results.push({
        url,
        status: 'success',
        result,
        timestamp: new Date().toISOString()
      });

      job.completed++;
    } catch (error) {
      console.error(chalk.red(`ì‹¤íŒ¨: ${url} - ${error.message}`));
      
      job.results.push({
        url,
        status: 'failed',
        error: error.message,
        timestamp: new Date().toISOString()
      });

      job.failed++;
    }
  }

  async saveJobReport(job) {
    const reportFile = path.join(BATCH_PROCESSOR_DIR, `job-${job.id}.json`);
    await fs.writeJson(reportFile, job, { spaces: 2 });
  }

  async scheduleJob(cronExpression, filePath, options = {}) {
    try {
      console.log(chalk.blue(`\nğŸ“… ìŠ¤ì¼€ì¤„ ì‘ì—… ë“±ë¡: ${cronExpression}\n`));

      const schedule = {
        id: `schedule-${Date.now()}`,
        cron: cronExpression,
        filePath,
        options,
        enabled: true,
        lastRun: null,
        nextRun: null,
        createdAt: new Date().toISOString()
      };

      // ì‹¤ì œë¡œëŠ” node-cron ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
      console.log(chalk.green(`âœ… ìŠ¤ì¼€ì¤„ ë“±ë¡ ì™„ë£Œ: ${schedule.id}\n`));

      return schedule;
    } catch (error) {
      console.error(chalk.red(`âŒ ìŠ¤ì¼€ì¤„ ë“±ë¡ ì‹¤íŒ¨: ${error.message}`));
      throw error;
    }
  }

  async getJobStatus(jobId) {
    const job = this.jobs.get(jobId);
    if (!job) {
      // íŒŒì¼ì—ì„œ ë¡œë“œ ì‹œë„
      const reportFile = path.join(BATCH_PROCESSOR_DIR, `job-${jobId}.json`);
      if (fs.existsSync(reportFile)) {
        return await fs.readJson(reportFile);
      }
      return null;
    }
    return job;
  }

  async listJobs() {
    const jobs = [];
    
    // ë©”ëª¨ë¦¬ì˜ ì‘ì—…
    for (const [id, job] of this.jobs.entries()) {
      jobs.push({
        id,
        status: job.status,
        total: job.total,
        completed: job.completed,
        failed: job.failed,
        startTime: job.startTime
      });
    }

    // íŒŒì¼ì˜ ì‘ì—…
    const files = await fs.readdir(BATCH_PROCESSOR_DIR);
    const jobFiles = files.filter(f => f.startsWith('job-') && f.endsWith('.json'));
    
    for (const file of jobFiles) {
      const jobId = path.basename(file, '.json');
      if (!this.jobs.has(jobId)) {
        const job = await fs.readJson(path.join(BATCH_PROCESSOR_DIR, file));
        jobs.push({
          id: jobId,
          status: job.status,
          total: job.total,
          completed: job.completed,
          failed: job.failed,
          startTime: job.startTime
        });
      }
    }

    return jobs;
  }
}

export default new BatchProcessor();

